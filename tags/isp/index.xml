<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Isp on All in Camera</title>
    <link>http://allincamera.github.io/tags/isp/</link>
    <description>Recent content in Isp on All in Camera</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 07 Apr 2016 13:32:07 +0800</lastBuildDate>
    <atom:link href="http://allincamera.github.io/tags/isp/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>反差对焦 vs. 相差对焦</title>
      <link>http://allincamera.github.io/post/cdaf_pdaf/</link>
      <pubDate>Thu, 07 Apr 2016 13:32:07 +0800</pubDate>
      
      <guid>http://allincamera.github.io/post/cdaf_pdaf/</guid>
      <description>

&lt;h2 id=&#34;反差式对焦评价函数的频率特性考虑:cfd866e8f4e163e652c7275fc6cb4e65&#34;&gt;反差式对焦评价函数的频率特性考虑&lt;/h2&gt;

&lt;p&gt;当前手机成像系统中普遍使用反差式对焦系统，也就是计算当前图像的锐度，依照图像锐度与镜头位置的关系，寻找最锐度最高的镜头位置作为合焦位置的方法。在搜索策略上各厂商基本相似，在锐度提取也就是清晰度的计算上，应该根据光学特性的不同做差异化设计。&lt;/p&gt;

&lt;p&gt;基于芯片硬件设计的方便性考虑，大部分的锐度评价函数都设计为一个m x n算子，记作P，算子的填充决定其特性：一般是高通，或者是带通滤波器。如果图像记作G，那么图像的锐度S可以表示为：
S = P * G  P 对 G 做卷积。&lt;/p&gt;

&lt;h3 id=&#34;高通与带通滤波器的优缺点:cfd866e8f4e163e652c7275fc6cb4e65&#34;&gt;高通与带通滤波器的优缺点&lt;/h3&gt;

&lt;p&gt;高通滤波器对高频成分很敏感，当成像系统离焦不远时，图像高频成分很容易被提取出来，随着镜头的移动，计算出锐度的差异很明显。但是当图像离焦很远时，比如对焦的物体消失，背景物体又在远处，高通滤波器对图像的响应就不明显，这样镜头移动计算出的锐度变化就不明显，造成搜索失败。&lt;/p&gt;

&lt;p&gt;如下图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/bandpass_filter.gif&#34; alt=&#34;Band Pass Filter&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当锐度值在曲线两侧，无论镜头如何移动，变化都非常不明显，这样搜索算法就很难工作，这个评价函数就是不合适的。为了解决这个问题，就需要在设计滤波器P的时候，让低频成分多通过一些。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/P1P2.gif&#34; alt=&#34;Low Pass Filter&#34; /&gt;&lt;/p&gt;

&lt;p&gt;蓝色的曲线是滤波器P1的频率响应，绿色的曲线是滤波器P2的频率响应，相比可见，绿色曲线可以让更多低频成分通过。&lt;/p&gt;

&lt;p&gt;选择一个滤波器后，需要根据实际图像进行计算仿真，画出这个滤波器对不同离焦程度图像卷积所产生的锐度值&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;	  kern = [-1 -2 -3 -5 -8;8 5 3 2 1];%c    
	%  kern = [-1  0  1; -2 0  2; -1  0  1];%r
	%     kern = [ -1 -1 -1;-1 0 1;1 1 1];%g
	%     kern = [ -1 -1 -1;-1 8 -1;-1 -1 -1];%b
	%    kern = [ 7 5 -1 0 -1 ;1 0 -5 1 -7]; %m
	     kern = [ 6 0 6 0 0 ;0 -12 0 0 0];%k
	%    kern = [-5 -4 0 4 5;-8 -10 0 10 8;10 -20 0 20 0;-8 -10 0 10 8;-5 -4 0 4 5]%y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/filters_comparison.gif&#34; alt=&#34;Filter Comparison&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图就是上面的各个滤波器的仿真结果，横坐标是离焦程度，从0到25，离焦程度逐步变大。纵坐标是计算出的锐度值，1表示最大。通过这个曲线，可以看出对不同离焦程度图像滤波器的响应，依据响应曲线的特点进行选择。&lt;/p&gt;

&lt;h2 id=&#34;片上相差式自动对焦-on-chip-phase-detection-autofocus-原理:cfd866e8f4e163e652c7275fc6cb4e65&#34;&gt;片上相差式自动对焦（on chip phase detection autofocus）原理&lt;/h2&gt;

&lt;p&gt;相差式自动对焦与反差式自动对焦是现在手机成像系统中两大主要自动对焦方式。相比反差式自动对焦，相差式自动对焦只需要一次计算，就可以完成对焦。&lt;/p&gt;

&lt;p&gt;当前比较流行的是片上相差自动对焦, 在生产sensor的时候，把某些用于相位检测像素遮住左边一半或者右边一半，如下图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/focus_pixel_array.png&#34; alt=&#34;Focus Pixels Array&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图只是示意图，各个厂商的半掩模的工艺各有不同，在对IR filter或者microlens的处理上也不相同，但是基本的原理都是让图像形成左右两幅类似人眼的不同光学通路的图像。&lt;/p&gt;

&lt;p&gt;这样左右侧的相位检测像素就会产生这样的图像：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/focus_pixel_sequence.png&#34; alt=&#34;Phase diff in focus pixels&#34; /&gt;&lt;/p&gt;

&lt;p&gt;数字化以后就产生了两个序列。&lt;/p&gt;

&lt;p&gt;图像聚焦时，两个序列做互相关运算产生的数值变小，图像离焦时，两个序列做互相关产生的数值变大。如果对相机模组进行校准&amp;mdash;-针对一个固定图形的高频图像移动镜头，计算互相关运算产生的数值，记录下来成为基准表。在相机工作时，根据实时计算的互相关数值，通过查找基准表，就可以知道当前的离焦程度，从而找到移动方向和移动到什么位置。&lt;/p&gt;

&lt;p&gt;数学推导简化起来就是如下公式：&lt;/p&gt;

&lt;p&gt;左右两个图像产生的数列做互相关，得到一个对焦函数，可以把相差与镜头的偏移量变成一一对应关系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/pdaf_func.png&#34; alt=&#34;PDAF func&#34; /&gt;&lt;/p&gt;

&lt;p&gt;实际工程上计算得到的结果就如下边图中所示，5x5窗口，每个窗口里边的统计数据包括两个部分，高16位是相位差，低16位是置信度。在平坦区域，置信度低，在细节丰富的区域，置信度高（300）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/pdaf_output.png&#34; alt=&#34;PDAF output&#34; /&gt;&lt;/p&gt;

&lt;p&gt;通过固定图卡校准可以得到lens 偏移量和相差的对应数组：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;PDAF_Calibration[][2] = {{1,1},{2,3},{3,5},{4,7},{5,9},{6,10},{7,11},{8,12},{9,13},{10,14},{11,15},{12,16},{13,17},{14,18},{15,19},{16,20}, {20,30},{24,40},{28,47},{32,50},{40,70},{48,80},{56,96},{64,110},{80,138},{96,160},{112,180},{128,210}};&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所以当AF开始工作时，通过实时计算得到相差值，eg： 210， 那么对应移动lens的距离，就是128,如果得到相差值是-210，就移动lens向反方向128个单位。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HDR Sensor 原理介绍</title>
      <link>http://allincamera.github.io/post/hdr_sensor_intro/</link>
      <pubDate>Tue, 05 Apr 2016 11:31:59 +0800</pubDate>
      
      <guid>http://allincamera.github.io/post/hdr_sensor_intro/</guid>
      <description>

&lt;p&gt;在介绍HDR Sensor原理之前首先讨论为什么需要HDR Sensor.&lt;/p&gt;

&lt;h2 id=&#34;什么是sensor的动态范围-dynamic-range:8adb1af61272cd03fe2458d7ffdec523&#34;&gt;什么是sensor的动态范围（dynamic range）？&lt;/h2&gt;

&lt;p&gt;sensor的动态范围就是sensor在一幅图像里能够同时体现高光和阴影部分内容的能力。
用公式表达这种能力就是：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;DR = 20log10（i_max / i_min); //dB&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;i_max 是sensor的最大不饱和电流&amp;mdash;-也可以说是sensor刚刚饱和时候的电流
i_min是sensor的底电流（blacklevel） ；&lt;/p&gt;

&lt;h2 id=&#34;为什么hdr在成像领域是个大问题:8adb1af61272cd03fe2458d7ffdec523&#34;&gt;为什么HDR在成像领域是个大问题？&lt;/h2&gt;

&lt;p&gt;在自然界的真实情况，有些场景的动态范围要大于100dB。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;人眼的动态范围可以达到100dB。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Sensor 的动态范围： &lt;strong&gt;高端的 &amp;gt;78 dB; 消费级的 60 dB 上下；&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;所以当sensor的动态范围小于图像场景动态范围的时候就会出现HDR问题&amp;mdash;-不是暗处看不清，就是亮处看不清，有的甚至两头都看不清。
&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/hdr_sensor_intro/dark_blur.png&#34; alt=&#34;Dark Blur Photo&#34; /&gt;
暗处看不清&amp;ndash;前景处的广告牌和树影太暗看不清。
&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/hdr_sensor_intro/bright_blur.png&#34; alt=&#34;Bright Blur Photo&#34; /&gt;
亮处看不清&amp;ndash;远处背景的白云变成了一团白色，完全看不清细节。&lt;/p&gt;

&lt;h2 id=&#34;解决hdr问题的数学分析:8adb1af61272cd03fe2458d7ffdec523&#34;&gt;解决HDR问题的数学分析&lt;/h2&gt;

&lt;p&gt;根据前边动态范围公式&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;DR = 20log10（i_max / i_min); //dB&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从数学本质上说要提高DR，就是提高i_max，减小 i_min；&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;对于10bit输出的sensor, i_max =1023，i_min =1, 动态范围DR = 60；&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对于12bit输出的sensor， DR = 72；&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以从数学上来看，提高sensor 输出的bit width就可以提高动态范围，从而解决HDR问题。可是现实上却没有这么简单。提高sensor的bit width导致不仅sensor的成本提高，整个图像处理器的带宽都得相应提高，消耗的内存也都相应提高，这样导致整个系统的成本会大幅提高。所以大家想出许多办法，既能解决HDR问题，又可以不增加太多成本。&lt;/p&gt;

&lt;h2 id=&#34;解决hdr问题的5种方法:8adb1af61272cd03fe2458d7ffdec523&#34;&gt;解决HDR问题的5种方法&lt;/h2&gt;

&lt;p&gt;从sensor的角度完整的DR 公式：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/hdr_sensor_intro/dr_formula.png&#34; alt=&#34;DR Formula&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Qsat ：Well Capacity   idc:  底电流，tint：曝光时间，σ:噪声。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;方法1-提高qsat-well-capacity:8adb1af61272cd03fe2458d7ffdec523&#34;&gt;方法1：提高Qsat  &amp;ndash;Well capacity 。&lt;/h3&gt;

&lt;p&gt;就是提高感光井的能力，这就涉及到sensor的构造，简单说，sensor的每个像素就像一口井，光子射到井里产生光电转换效应，井的容量如果比较大，容纳的电荷就比较多，这样i_max的值就更大。普通的sensor well只reset一次，但是为了提高动态范围，就产生了多次reset的方法。
通过多次reset，imax增加到i‘max，上图就是current to charge的转换曲线。
但这种方法的缺点是增加FPN，而且sensor的响应变成非线性，后边的处理会增加难度。&lt;/p&gt;

&lt;h3 id=&#34;方法2-多曝光合成:8adb1af61272cd03fe2458d7ffdec523&#34;&gt;方法2：多曝光合成&lt;/h3&gt;

&lt;p&gt;本质上这种方法就是用短曝光获取高光处的图像，用长曝光获取阴暗处的图像。有的厂家用前后两帧长短曝光图像，或者前后三针长、中、短曝光图像进行融合&lt;/p&gt;

&lt;p&gt;&amp;rdquo;&amp;rsquo;
    If (Intensity &amp;gt; a) intensity = short_exposure_frame;
    If (Intensity &amp;lt; b) intensity = long_exposure_frame;
    If (b&amp;lt;Intensity &amp;lt;a) intensity = long_exposure_frame x p + short_exposure_frame x q;
&amp;ldquo;&amp;rsquo;&lt;/p&gt;

&lt;p&gt;当该像素值大于一个门限时，这个像素的数值就是来自于短曝光，小于一个数值，该像素值就来自于长曝光，在中间的话，就用长短曝光融合。这是个比较简化的方法，实际上还要考虑噪声等的影响。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/hdr_sensor_intro/curve_multi_frame_current.png&#34; alt=&#34;Curve Multi Frame Current&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Current to charge曲线显示：imax增加a倍。
这种多帧融合的方法需要非常快的readout time，而且即使readout时间再快，多帧图像也会有时间差，所以很难避免在图像融合时产生的鬼影问题。尤其在video HDR的时候，由于运算时间有限，无法进行复杂的去鬼影的运算，会有比较明显的问题。于是就出现了单帧的多曝光技术。&lt;/p&gt;

&lt;h3 id=&#34;方法3-单帧空间域多曝光:8adb1af61272cd03fe2458d7ffdec523&#34;&gt;方法3：单帧空间域多曝光。&lt;/h3&gt;

&lt;p&gt;最开始的方法是在sensor的一些像素上加ND filter，让这些像素获得的光强度变弱，所以当其他正常像素饱和的时候，这些像素仍然没有饱和，不过这样做生产成本比较高，同时给后边的处理增加很多麻烦。所以下面的这种隔行多曝光方法更好些。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/hdr_sensor_intro/single_frame_multi_exp.png&#34; alt=&#34;Single Frame Multi Exposure&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如上图所示,两行短曝光，再两行长曝光,然后做图像融合，这样可以较好的避免多帧融合的问题，从而有效的在video中实现HDR。同时由于video的分辨率比still要低很多，所以这个方法所产生的分辨率降低也不是问题。这个方法是现在video hdr sensor的主流技术。&lt;/p&gt;

&lt;h3 id=&#34;方法4-logarithmic-sensor:8adb1af61272cd03fe2458d7ffdec523&#34;&gt;方法4：logarithmic sensor&lt;/h3&gt;

&lt;p&gt;实际是一种数学方法，把图像从线性域压缩到log域，从而压缩了动态范围，在数字通信里也用类似的技术使用不同的函数进行压缩，在isp端用反函数再恢复到线性，再做信号处理。&lt;/p&gt;

&lt;p&gt;缺点一方面是信号不是线性的，另一方面会增加FPN，同时由于压缩精度要求对硬件设计要求高。&lt;/p&gt;

&lt;h3 id=&#34;方法5-局部适应-local-adaption:8adb1af61272cd03fe2458d7ffdec523&#34;&gt;方法5：局部适应 local adaption&lt;/h3&gt;

&lt;p&gt;这是种仿人眼的设计，人眼会针对局部的图像特点进行自适应，既能够增加局部的对比度，同时保留大动态范围。这种算法比较复杂，有很多论文单独讨论。目前在sensor 端还没有使用这种技术，在ISP和后处理这种方法已经得到了非常好的应用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/hdr_sensor_intro/2_5_hdr.png&#34; alt=&#34;After HDR&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图就是用方法2 + 方法5处理后的HDR图像。亮处与暗处的细节都得到了很好的展现。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>