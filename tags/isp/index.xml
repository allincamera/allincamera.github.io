<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Isp on All in Camera</title>
    <link>http://allincamera.github.io/tags/isp/</link>
    <description>Recent content in Isp on All in Camera</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 30 Apr 2016 00:44:57 +0800</lastBuildDate>
    <atom:link href="http://allincamera.github.io/tags/isp/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>【科普】双摄像头能做什么？</title>
      <link>http://allincamera.github.io/post/dual_cam_intro_01/</link>
      <pubDate>Sat, 30 Apr 2016 00:44:57 +0800</pubDate>
      
      <guid>http://allincamera.github.io/post/dual_cam_intro_01/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;本文系微信公众号『大话成像』，知乎专栏『all in camera』原创文章，转载请注明出处并保留本声明&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;目前主流的双摄像头的功能。主要可以分为两大类：
1. 利用双摄像头产生立体视觉，获得影像的景深。以及利用将不同的景深进行3D建模，图像处理，物体分割，物体识别和跟踪，对焦辅助之类的功能
2. 利用左右两张不同的图片信息进行融合，以期望得到更高的分辨率，更好的色彩，动态范围等更好的图像质量。&lt;/p&gt;

&lt;p&gt;这两类双摄像头功能对于摄像头的硬件有着不同的要求，前者要求两个摄像头得到像差尽量大，这样能够得到的景深精度更高，因此前者的硬件希望两个摄像头间的距离比较远才好。而后者希望两个摄像头的像在空间和时间上都尽量能够接近，因此在硬件设计的时候希望两个摄像头离得比较近。这样在两个图像融合的时候才不会因为相差产生更多的错误。&lt;/p&gt;

&lt;p&gt;但是由于摄像头无法做的完全一致。因此无论是这两类功能的哪一类，算法都希望能在得到图像的同时，能够更多的得到硬件的实际情况如：姿势差和两个摄像头的镜头畸变等。而这些信息需要平台算法和模组生产手机生产使用相同且方便于工程化的算法进行计算，很多牵扯到硬件本身的特性，不是简单从理论计算就能解决的。以后有时间我们会继续讨论这个话题。总之双摄像头的使用过程中算法和硬件本身结合的十分紧密，不可分割。因此说到双摄像头现在首先应该关注的应该是双摄像头能带来什么样的用户体验。双摄像头中对软件硬件结合的要求远比单摄像头要高。而我们在看到一款使用双摄像头的手机的时候从它的硬件设计就能看出它是偏重于哪一类功能。下图就是常见的两个摄像头在姿势差上面的信息。就是针对3D坐标轴XYZ的平移和旋转。后面面我们把双摄像头的能实现功能给大家介绍。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/dual_cam_intro_01/dualcam_module.png&#34; alt=&#34;Dual Cam Module&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;景深应用:681b06908344f27af169accd314408b0&#34;&gt;景深应用&lt;/h2&gt;

&lt;p&gt;第一类功能首先要获得当前场景的景深图。其基本原理就是基于三角定位。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/dual_cam_intro_01/depth_calc.jpg&#34; alt=&#34;Depth Calc&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;背景虚化:681b06908344f27af169accd314408b0&#34;&gt;背景虚化&lt;/h3&gt;

&lt;p&gt;第一类功能中最典型的就是背景虚化，在景深图的基础上将不同距离的物体进行虚化，模拟大光圈相机拍摄效果。&lt;/p&gt;

&lt;p&gt;该功能在以前的双camera中经常能见到，如HTC的后置双摄和联想前置双摄S1。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/dual_cam_intro_01/depth_blur.jpg&#34; alt=&#34;Depth Blur&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;物体分割:681b06908344f27af169accd314408b0&#34;&gt;物体分割&lt;/h3&gt;

&lt;p&gt;目前在手机上主要用于图片的裁减，和背景替换，利用景深信息可以更好的可以将不同景深的物体分割开。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/dual_cam_intro_01/depth_split.png&#34; alt=&#34;Depth Split&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;3d扫描:681b06908344f27af169accd314408b0&#34;&gt;3D扫描&lt;/h3&gt;

&lt;p&gt;通过不同角度下的景深图进行建模。这对景深图和算法的要求比较高。手机上的双camera之间的距离有限往往能够得到的景深图的精度不够好，景深信息处理算法复杂在手机硬件上计算的时间长，因此目前这个功能在手机上面基本没有。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/dual_cam_intro_01/3d_model.png&#34; alt=&#34;3D Scanning&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;目标物体距离计算和快速对焦:681b06908344f27af169accd314408b0&#34;&gt;目标物体距离计算和快速对焦&lt;/h3&gt;

&lt;p&gt;利用三角定位计算景深的最简单的应用&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/dual_cam_intro_01/fast_focus.png&#34; alt=&#34;fast focus&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;3d视频及照片制作:681b06908344f27af169accd314408b0&#34;&gt;3D视频及照片制作&lt;/h3&gt;

&lt;p&gt;不同于一般的3D电影的拍摄。手机上的两个摄像头无法在图像的拍摄过程中就产生足够的视觉差，这是由于两个摄像头中间的距离和人眼不一样。而且为了能够让人们更明显的得到3D视觉效果。所以往往需要算法进行增强。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/dual_cam_intro_01/3d_video.jpg&#34; alt=&#34;3D Photo&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;ar增强和动作识别:681b06908344f27af169accd314408b0&#34;&gt;AR增强和动作识别&lt;/h3&gt;

&lt;p&gt;主要是利用两个摄像头进行手势或者姿势的识别。目前在市场上比较常见的就是leap motion 还有微软的 Kinect 都是类似的功能。亚马逊fire phone 曾经尝试在手机上实现类似的功能，但是最终手机的供电以及空间并没有给用户良好的体验。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/dual_cam_intro_01/3d_gesture.jpg&#34; alt=&#34;3D Gesture&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;图像合成:681b06908344f27af169accd314408b0&#34;&gt;图像合成&lt;/h2&gt;

&lt;p&gt;第二类功能都是通过将不同的图片中的不同的信息，合成到一张图片中，使合成之后的图片得到更好的效果。此类应用硬件设计中就会注意如何分别提供不同的信息。&lt;/p&gt;

&lt;h3 id=&#34;超分辨率:681b06908344f27af169accd314408b0&#34;&gt;超分辨率&lt;/h3&gt;

&lt;p&gt;主要是利用多张图片中在高频部分不同的内容生成一张清晰的图片，双camera可以通过两张照片中不同的信息进行最后的增强。&lt;/p&gt;

&lt;p&gt;然而传统算法生成需要的图片一般都是需要更多的图片，两张图片能够提供的不同信息还是太少，如华为的Mate6 plus号称是两颗8M可以合成13M的图像，但是实际拍摄的图像的解析力还只是8M的水平，和我们自己缩放一张图片并没有什么区别。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;有关解析力的区别可参阅往期解析力测试相关内容&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/dual_cam_intro_01/super_resolution.jpg&#34; alt=&#34;Super Resolution&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;hdr:681b06908344f27af169accd314408b0&#34;&gt;HDR&lt;/h3&gt;

&lt;p&gt;对两个Camera设置不同的曝光参数以得到不同曝光参数的图像进行HDR合成。&lt;/p&gt;

&lt;p&gt;以往这个功能需要通过以往一个摄像头去修改曝光时间来得到不同曝光情况下的图片，但是这种办法需要的时间长。这不仅导致了用户体验变差，且如果场景中有运动物体或者相机有移动的话会导致鬼影的问题。利用双camera则能避免类似的问题。但是大多数合成的过程中多数的HDR算法主要关注于亮度信息，多数的算法在颜色方面会有些失真。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/dual_cam_intro_01/hdr.wm.png&#34; alt=&#34;HDR Sample&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;低光提亮和去噪:681b06908344f27af169accd314408b0&#34;&gt;低光提亮和去噪&lt;/h3&gt;

&lt;p&gt;低光提亮和去噪在算法上和HDR基本没有区别。主要是利用两颗摄像头中一颗&lt;strong&gt;黑白摄像头&lt;/strong&gt;的低光下响应较好噪声较小的特性。&lt;/p&gt;

&lt;p&gt;其优点是对于彩噪有不错的抑制性一般能够3个DB左右的SNR提高，但是实际拍摄的图片多数情况下效果提升有限，和有些在低光下做过特殊处理和tuning的单camera系统比并没有很明显的优势。&lt;/p&gt;

&lt;p&gt;华为P9这一次主要就采用了这种camera的设计，从多数我们看到的网上的评价来看目前在低光下的效果有改进但并不惊艳。在华为P9之前，奇酷也采用类似的硬件设计实现这个功能。&lt;/p&gt;

&lt;p&gt;这个功能目前看给用户的提升很有限，也许后面还有挖掘的潜力。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/dual_cam_intro_01/low_light.wm.png&#34; alt=&#34;Low Light&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;光学变焦:681b06908344f27af169accd314408b0&#34;&gt;光学变焦&lt;/h3&gt;

&lt;p&gt;最近比较好的应用趋势是利用一颗正常的FOV的摄像头模组和一颗远焦镜头的模组达到光学变焦的效果。&lt;/p&gt;

&lt;p&gt;从苹果收购以色列的Linx公司还有苹果公司最新公布的专利来看，水果公司最新的专利来看这个功能很可能是水果公司的双camera的主要功能。远焦镜头的视场角要比正常的镜头小很多，但是相同距离下图片的解析力也会高不少。利用远焦镜头可以提供的较好的分辨率既可以在平时拍照的时候利用融合算法提高中心区域的分辨率，也可以在变焦时利用远焦的照片提高ZOOM后的解析力。从之前看到过的另外一家以色列公司CP的算法效果来看，如果手机上实现的好的话可能在中心区域超过现有手机摄像头最大的解析力，在效果上和光学zoom相差不大。虽然这个功能在硬件设计的时候有着远焦模组过高的问题，但是这依然是第二类功能中目前能看到效果最好的功能。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/dual_cam_intro_01/optical_zoom.jpg&#34; alt=&#34;Optical Zoom&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;现状总结:681b06908344f27af169accd314408b0&#34;&gt;现状总结&lt;/h2&gt;

&lt;p&gt;从目前市面上能看到双Camera手机来看：&lt;/p&gt;

&lt;p&gt;偏景深类功能的手机目前主要的问题是景深信息不够准确计算速度慢。无法实时得到景深信息。不过目前手机硬件的发展速度很快，很多ISP的设计已经开始考虑双camera的实时景深。今年很多实时景深的ISP将开始在平台端普及，虽然分辨率还不是很理想。但是随着实时景深的普及，景深信息的分辨率的增加，相信后面利用景深信息实现的场景物体识别类的功能相信会越来越多。类似我们之前看电影中的一些现实增强的效果我相信 也会越来越多。而且这类功能在智能机器人领域也是目前的热门。如陪李总理的打球的机械人就是这类功能的延伸。不过进入平常百姓家这个肯定需要时间。&lt;/p&gt;

&lt;p&gt;而偏第二类图片融合得到更好图像质量的功能，从目前已有的手机来看都不是很成功。不过实现光学变焦的效果从算法效果的层面上来看效果很不错,或许苹果的双摄出来之后能在这个领域带起一个风潮。&lt;/p&gt;

&lt;p&gt;两类应用对摄像头模组要求是不同的，前者要求模组间的距离较大为好，后者则要求模组间的距离尽量小。虽然无论做的多小但由于模组本身的尺寸，两个摄像头间肯定会有些距离的。比如华为P9两个摄像头间的距离已经小于1厘米，但是再想做的更小可能需要突破性的设计。&lt;/p&gt;

&lt;p&gt;当然，为其中一类功能设计的双摄像头模组也可以实现另外一类功能，只是勉强实现会增加算法的复杂程度，在最终实现的效果上也会受到很大制约。比如P9虽然也实现了景深功能但效果和专门为景深功能功能设计的双摄像头比还是很有差距的。&lt;/p&gt;

&lt;p&gt;双摄像头的风潮刚刚开始，后续我们会邀请不同领域的专家来介绍双摄像头实现的一些具体问题。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;本文系微信公众号『大话成像』，知乎专栏『all in camera』原创文章，转载请注明出处并保留本声明&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>反差式对焦评价函数的频率特性考虑</title>
      <link>http://allincamera.github.io/post/cdaf/</link>
      <pubDate>Thu, 07 Apr 2016 13:32:07 +0800</pubDate>
      
      <guid>http://allincamera.github.io/post/cdaf/</guid>
      <description>

&lt;p&gt;当前手机成像系统中普遍使用反差式对焦系统，也就是计算当前图像的锐度，依照图像锐度与镜头位置的关系，寻找最锐度最高的镜头位置作为合焦位置的方法。在搜索策略上各厂商基本相似，在锐度提取也就是清晰度的计算上，应该根据光学特性的不同做差异化设计。&lt;/p&gt;

&lt;p&gt;基于芯片硬件设计的方便性考虑，大部分的锐度评价函数都设计为一个m x n算子，记作P，算子的填充决定其特性：一般是高通，或者是带通滤波器。如果图像记作G，那么图像的锐度S可以表示为：
S = P * G  P 对 G 做卷积。&lt;/p&gt;

&lt;h3 id=&#34;高通与带通滤波器的优缺点:21488b96ffcd9dfd757627af1ef7cae6&#34;&gt;高通与带通滤波器的优缺点&lt;/h3&gt;

&lt;p&gt;高通滤波器对高频成分很敏感，当成像系统离焦不远时，图像高频成分很容易被提取出来，随着镜头的移动，计算出锐度的差异很明显。但是当图像离焦很远时，比如对焦的物体消失，背景物体又在远处，高通滤波器对图像的响应就不明显，这样镜头移动计算出的锐度变化就不明显，造成搜索失败。&lt;/p&gt;

&lt;p&gt;如下图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/bandpass_filter.gif&#34; alt=&#34;Band Pass Filter&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当锐度值在曲线两侧，无论镜头如何移动，变化都非常不明显，这样搜索算法就很难工作，这个评价函数就是不合适的。为了解决这个问题，就需要在设计滤波器P的时候，让低频成分多通过一些。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/P1P2.gif&#34; alt=&#34;Low Pass Filter&#34; /&gt;&lt;/p&gt;

&lt;p&gt;蓝色的曲线是滤波器P1的频率响应，绿色的曲线是滤波器P2的频率响应，相比可见，绿色曲线可以让更多低频成分通过。&lt;/p&gt;

&lt;p&gt;选择一个滤波器后，需要根据实际图像进行计算仿真，画出这个滤波器对不同离焦程度图像卷积所产生的锐度值&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;	  kern = [-1 -2 -3 -5 -8;8 5 3 2 1];%c    
	%  kern = [-1  0  1; -2 0  2; -1  0  1];%r
	%     kern = [ -1 -1 -1;-1 0 1;1 1 1];%g
	%     kern = [ -1 -1 -1;-1 8 -1;-1 -1 -1];%b
	%    kern = [ 7 5 -1 0 -1 ;1 0 -5 1 -7]; %m
	     kern = [ 6 0 6 0 0 ;0 -12 0 0 0];%k
	%    kern = [-5 -4 0 4 5;-8 -10 0 10 8;10 -20 0 20 0;-8 -10 0 10 8;-5 -4 0 4 5]%y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/filters_comparison.gif&#34; alt=&#34;Filter Comparison&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图就是上面的各个滤波器的仿真结果，横坐标是离焦程度，从0到25，离焦程度逐步变大。纵坐标是计算出的锐度值，1表示最大。通过这个曲线，可以看出对不同离焦程度图像滤波器的响应，依据响应曲线的特点进行选择。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>片上相差式自动对焦原理</title>
      <link>http://allincamera.github.io/post/pdaf/</link>
      <pubDate>Thu, 07 Apr 2016 13:32:07 +0800</pubDate>
      
      <guid>http://allincamera.github.io/post/pdaf/</guid>
      <description>&lt;p&gt;相差式自动对焦与反差式自动对焦是现在手机成像系统中两大主要自动对焦方式。相比反差式自动对焦，相差式自动对焦只需要一次计算，就可以完成对焦。&lt;/p&gt;

&lt;p&gt;当前比较流行的是片上相差自动对焦（on chip phase detection autofocus）, 在生产sensor的时候，把某些用于相位检测像素遮住左边一半或者右边一半，如下图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/focus_pixel_array.png&#34; alt=&#34;Focus Pixels Array&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图只是示意图，各个厂商的半掩模的工艺各有不同，在对IR filter或者microlens的处理上也不相同，但是基本的原理都是让图像形成左右两幅类似人眼的不同光学通路的图像。&lt;/p&gt;

&lt;p&gt;这样左右侧的相位检测像素就会产生这样的图像：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/focus_pixel_sequence.png&#34; alt=&#34;Phase diff in focus pixels&#34; /&gt;&lt;/p&gt;

&lt;p&gt;数字化以后就产生了两个序列。&lt;/p&gt;

&lt;p&gt;图像聚焦时，两个序列做互相关运算产生的数值变小，图像离焦时，两个序列做互相关产生的数值变大。如果对相机模组进行校准&amp;mdash;-针对一个固定图形的高频图像移动镜头，计算互相关运算产生的数值，记录下来成为基准表。在相机工作时，根据实时计算的互相关数值，通过查找基准表，就可以知道当前的离焦程度，从而找到移动方向和移动到什么位置。&lt;/p&gt;

&lt;p&gt;数学推导简化起来就是如下公式：&lt;/p&gt;

&lt;p&gt;左右两个图像产生的数列做互相关，得到一个对焦函数，可以把相差与镜头的偏移量变成一一对应关系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/pdaf_func.png&#34; alt=&#34;PDAF func&#34; /&gt;&lt;/p&gt;

&lt;p&gt;实际工程上计算得到的结果就如下边图中所示，5x5窗口，每个窗口里边的统计数据包括两个部分，高16位是相位差，低16位是置信度。在平坦区域，置信度低，在细节丰富的区域，置信度高（300）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/pdaf_output.png&#34; alt=&#34;PDAF output&#34; /&gt;&lt;/p&gt;

&lt;p&gt;通过固定图卡校准可以得到lens 偏移量和相差的对应数组：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;PDAF_Calibration[][2] = {{1,1},{2,3},{3,5},{4,7},{5,9},{6,10},{7,11},{8,12},{9,13},{10,14},{11,15},{12,16},{13,17},{14,18},{15,19},{16,20}, {20,30},{24,40},{28,47},{32,50},{40,70},{48,80},{56,96},{64,110},{80,138},{96,160},{112,180},{128,210}};&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所以当AF开始工作时，通过实时计算得到相差值，eg： 210， 那么对应移动lens的距离，就是128,如果得到相差值是-210，就移动lens向反方向128个单位。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HDR Sensor 原理介绍</title>
      <link>http://allincamera.github.io/post/hdr_sensor_intro/</link>
      <pubDate>Tue, 05 Apr 2016 11:31:59 +0800</pubDate>
      
      <guid>http://allincamera.github.io/post/hdr_sensor_intro/</guid>
      <description>

&lt;p&gt;在介绍HDR Sensor原理之前首先讨论为什么需要HDR Sensor.&lt;/p&gt;

&lt;h2 id=&#34;什么是sensor的动态范围-dynamic-range:8adb1af61272cd03fe2458d7ffdec523&#34;&gt;什么是sensor的动态范围（dynamic range）？&lt;/h2&gt;

&lt;p&gt;sensor的动态范围就是sensor在一幅图像里能够同时体现高光和阴影部分内容的能力。
用公式表达这种能力就是：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;DR = 20log10（i_max / i_min); //dB&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;i_max 是sensor的最大不饱和电流&amp;mdash;-也可以说是sensor刚刚饱和时候的电流
i_min是sensor的底电流（blacklevel） ；&lt;/p&gt;

&lt;h2 id=&#34;为什么hdr在成像领域是个大问题:8adb1af61272cd03fe2458d7ffdec523&#34;&gt;为什么HDR在成像领域是个大问题？&lt;/h2&gt;

&lt;p&gt;在自然界的真实情况，有些场景的动态范围要大于100dB。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;人眼的动态范围可以达到100dB。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Sensor 的动态范围： &lt;strong&gt;高端的 &amp;gt;78 dB; 消费级的 60 dB 上下；&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;所以当sensor的动态范围小于图像场景动态范围的时候就会出现HDR问题&amp;mdash;-不是暗处看不清，就是亮处看不清，有的甚至两头都看不清。
&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/hdr_sensor_intro/dark_blur.png&#34; alt=&#34;Dark Blur Photo&#34; /&gt;
暗处看不清&amp;ndash;前景处的广告牌和树影太暗看不清。
&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/hdr_sensor_intro/bright_blur.png&#34; alt=&#34;Bright Blur Photo&#34; /&gt;
亮处看不清&amp;ndash;远处背景的白云变成了一团白色，完全看不清细节。&lt;/p&gt;

&lt;h2 id=&#34;解决hdr问题的数学分析:8adb1af61272cd03fe2458d7ffdec523&#34;&gt;解决HDR问题的数学分析&lt;/h2&gt;

&lt;p&gt;根据前边动态范围公式&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;DR = 20log10（i_max / i_min); //dB&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从数学本质上说要提高DR，就是提高i_max，减小 i_min；&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;对于10bit输出的sensor, i_max =1023，i_min =1, 动态范围DR = 60；&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对于12bit输出的sensor， DR = 72；&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以从数学上来看，提高sensor 输出的bit width就可以提高动态范围，从而解决HDR问题。可是现实上却没有这么简单。提高sensor的bit width导致不仅sensor的成本提高，整个图像处理器的带宽都得相应提高，消耗的内存也都相应提高，这样导致整个系统的成本会大幅提高。所以大家想出许多办法，既能解决HDR问题，又可以不增加太多成本。&lt;/p&gt;

&lt;h2 id=&#34;解决hdr问题的5种方法:8adb1af61272cd03fe2458d7ffdec523&#34;&gt;解决HDR问题的5种方法&lt;/h2&gt;

&lt;p&gt;从sensor的角度完整的DR 公式：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/hdr_sensor_intro/dr_formula.png&#34; alt=&#34;DR Formula&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Qsat ：Well Capacity   idc:  底电流，tint：曝光时间，σ:噪声。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;方法1-提高qsat-well-capacity:8adb1af61272cd03fe2458d7ffdec523&#34;&gt;方法1：提高Qsat  &amp;ndash;Well capacity 。&lt;/h3&gt;

&lt;p&gt;就是提高感光井的能力，这就涉及到sensor的构造，简单说，sensor的每个像素就像一口井，光子射到井里产生光电转换效应，井的容量如果比较大，容纳的电荷就比较多，这样i_max的值就更大。普通的sensor well只reset一次，但是为了提高动态范围，就产生了多次reset的方法。
通过多次reset，imax增加到i‘max，上图就是current to charge的转换曲线。
但这种方法的缺点是增加FPN，而且sensor的响应变成非线性，后边的处理会增加难度。&lt;/p&gt;

&lt;h3 id=&#34;方法2-多曝光合成:8adb1af61272cd03fe2458d7ffdec523&#34;&gt;方法2：多曝光合成&lt;/h3&gt;

&lt;p&gt;本质上这种方法就是用短曝光获取高光处的图像，用长曝光获取阴暗处的图像。有的厂家用前后两帧长短曝光图像，或者前后三针长、中、短曝光图像进行融合&lt;/p&gt;

&lt;p&gt;&amp;rdquo;&amp;rsquo;
    If (Intensity &amp;gt; a) intensity = short_exposure_frame;
    If (Intensity &amp;lt; b) intensity = long_exposure_frame;
    If (b&amp;lt;Intensity &amp;lt;a) intensity = long_exposure_frame x p + short_exposure_frame x q;
&amp;ldquo;&amp;rsquo;&lt;/p&gt;

&lt;p&gt;当该像素值大于一个门限时，这个像素的数值就是来自于短曝光，小于一个数值，该像素值就来自于长曝光，在中间的话，就用长短曝光融合。这是个比较简化的方法，实际上还要考虑噪声等的影响。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/hdr_sensor_intro/curve_multi_frame_current.png&#34; alt=&#34;Curve Multi Frame Current&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Current to charge曲线显示：imax增加a倍。
这种多帧融合的方法需要非常快的readout time，而且即使readout时间再快，多帧图像也会有时间差，所以很难避免在图像融合时产生的鬼影问题。尤其在video HDR的时候，由于运算时间有限，无法进行复杂的去鬼影的运算，会有比较明显的问题。于是就出现了单帧的多曝光技术。&lt;/p&gt;

&lt;h3 id=&#34;方法3-单帧空间域多曝光:8adb1af61272cd03fe2458d7ffdec523&#34;&gt;方法3：单帧空间域多曝光。&lt;/h3&gt;

&lt;p&gt;最开始的方法是在sensor的一些像素上加ND filter，让这些像素获得的光强度变弱，所以当其他正常像素饱和的时候，这些像素仍然没有饱和，不过这样做生产成本比较高，同时给后边的处理增加很多麻烦。所以下面的这种隔行多曝光方法更好些。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/hdr_sensor_intro/single_frame_multi_exp.png&#34; alt=&#34;Single Frame Multi Exposure&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如上图所示,两行短曝光，再两行长曝光,然后做图像融合，这样可以较好的避免多帧融合的问题，从而有效的在video中实现HDR。同时由于video的分辨率比still要低很多，所以这个方法所产生的分辨率降低也不是问题。这个方法是现在video hdr sensor的主流技术。&lt;/p&gt;

&lt;h3 id=&#34;方法4-logarithmic-sensor:8adb1af61272cd03fe2458d7ffdec523&#34;&gt;方法4：logarithmic sensor&lt;/h3&gt;

&lt;p&gt;实际是一种数学方法，把图像从线性域压缩到log域，从而压缩了动态范围，在数字通信里也用类似的技术使用不同的函数进行压缩，在isp端用反函数再恢复到线性，再做信号处理。&lt;/p&gt;

&lt;p&gt;缺点一方面是信号不是线性的，另一方面会增加FPN，同时由于压缩精度要求对硬件设计要求高。&lt;/p&gt;

&lt;h3 id=&#34;方法5-局部适应-local-adaption:8adb1af61272cd03fe2458d7ffdec523&#34;&gt;方法5：局部适应 local adaption&lt;/h3&gt;

&lt;p&gt;这是种仿人眼的设计，人眼会针对局部的图像特点进行自适应，既能够增加局部的对比度，同时保留大动态范围。这种算法比较复杂，有很多论文单独讨论。目前在sensor 端还没有使用这种技术，在ISP和后处理这种方法已经得到了非常好的应用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/allincamera/imgur/master/hdr_sensor_intro/2_5_hdr.png&#34; alt=&#34;After HDR&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图就是用方法2 + 方法5处理后的HDR图像。亮处与暗处的细节都得到了很好的展现。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>