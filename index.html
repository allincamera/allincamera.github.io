<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.15" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> All in Camera &middot; All in Camera </title>

  
  <link rel="stylesheet" href="http://allincamera.github.io/css/poole.css">
  <link rel="stylesheet" href="http://allincamera.github.io/css/syntax.css">
  <link rel="stylesheet" href="http://allincamera.github.io/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="http://allincamera.github.io/index.xml" rel="alternate" type="application/rss+xml" title="All in Camera" />
</head>

<body class=" ">

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="http://allincamera.github.io/"><h1>All in Camera</h1></a>
      <p class="lead">
      An elegant open source and mobile first theme for <a href="http://hugo.spf13.com">hugo</a> made by <a href="http://twitter.com/mdo">@mdo</a>. Originally made for Jekyll.
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="/">Home</a> </li>
      
    </ul>

    <p>&copy; 2016. All rights reserved. </p>
  </div>
</div>


    <div class="content container">
<div class="posts">

      
  <div class="post">
    <h1 class="post-title">
      <a href="http://allincamera.github.io/post/mtf_test_intro/">
        镜头MTF测试解析
      </a>
    </h1>

    <span class="post-date">Mon, Apr 11, 2016</span>

    

<p>本文特邀在镜头制造行业资深专家Jerry Li为大话成像（All in camera ）撰写。</p>

<h2 id="引言:754679329a565ba3bd547f760dbb0738">引言</h2>

<p>成像清晰度是镜头优劣的重要指标之一，在镜头制造业笔者见过简易的逆投影以及更精准的镜头MTF测试，本文要讲述的是后者设备测试原理及如何通过测试数据判断镜头的品质。</p>

<p>测量MTF的设备，首先需要一个极微小发光体，通过被检测镜头成像，然后选择合适的高倍物镜放大并成像到设备CCD上，将图像传输到电脑用专用软件分析光强度分布，最终通过傅里叶变换计算得出MTF。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/mtf_test_intro/mtf_test_setup.png" alt="MTF Test Setup" /></p>

<h2 id="测试光源:754679329a565ba3bd547f760dbb0738">测试光源</h2>

<p>MTF 测试中常用三种发光体：点，狭缝，刃边。以下将逐一介绍如何用这三种光源。</p>

<h3 id="点光源:754679329a565ba3bd547f760dbb0738">点光源</h3>

<p>一个理想的点光源可以看成是XY方向上无限小的物体，其能量分布用二维脉冲函数δ(x, y)来表示。理想的点光源经过由像差的光学系统后，所成的像会形成一个弥散斑，其光强分布即光学系统的脉冲响应，也被称作点扩散函数PSF(x, y)。如下图所示。 </p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/mtf_test_intro/point_light_source.png" alt="Point Light source Image" /></p>

<p>我们用一个与位置有关的函数h(x, y)来表示脉冲响应的光强分布，用“*”表示成像过程的卷积操作，则一个理想输入f(x, y)经过光学系统成像后再像面的强度分布g(x, y)可以表示成：</p>

<blockquote>
<p>g(x, y) = f(x, y)*h(x, y)</p>
</blockquote>

<p>对上式两端分别进行二维傅里叶变换，有</p>

<blockquote>
<p>G(fx, fy) = F(fx, fy)H(fx, fy)</p>
</blockquote>

<p>式中，G(fx, fy)，F(fx, fy)和H(fx, fy)分别是g(x, y)，f(x, y)和h(x, y)的傅里叶变换，fx和fy是频域中沿两个坐标方向的空间频率。函数H(fx, fy)就是我们要求得到的光学传递函数（OTF）。OTF是一个包括实数和虚数两部分的复变函数，可以写成下述公式：</p>

<blockquote>
<p>OTF(fx, fy) = |H(fx, fy)|expφ(fx, fy)</p>
</blockquote>

<p>其中，实数部分|H(fx, fy)|就是我们要得到的MTF。</p>

<h3 id="狭缝光源:754679329a565ba3bd547f760dbb0738">狭缝光源</h3>

<p>因为点光源提供的能量较弱，而且得到理想的点光源也比较困难，所以常用的方法是用狭缝来产生线光源。如下图，多个点光源（间隔无限小）假设沿y方向排列形成一维光源，各发光点不相干，则等效狭缝可以看成y方向为常量，以x为变量的delta函数。可以用下式表示：</p>

<blockquote>
<p>f(x,y) = δ(x)l(y)</p>
</blockquote>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/mtf_test_intro/slit_line_light_source.png" alt="Slit Line light source" /></p>

<p>狭缝的沿X方向的光强度分布就是线扩散函数LSF，所以狭缝成像的光强度分布g(x, y)可以表示成：</p>

<blockquote>
<p>g(x, y) = LSF(x) = f(x, y)*h(x, y) = [δ(x)l(y)]*PSF(x, y)</p>
</blockquote>

<h3 id="刃边光源:754679329a565ba3bd547f760dbb0738">刃边光源</h3>

<p>如果在某些条件下狭缝提供的能量还是不够，那么就需要用到刃边作为光源体，通过边缘扩散函数ESF求导得到LSF，间接计算出MTF。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/mtf_test_intro/esf_lsf_mtf.png" alt="Edge Method" /></p>

<p>点光源、狭缝、刃边的测试方法除了提供能量不同之外，差异还包括点光源可以同时计算任意角度的多个方向的MTF，狭缝和刃边一次只能计算一个方向的MTF。由于CCD采集信号噪声的影响，狭缝相比点光源有更高的测量精度，刃边比狭缝多了一次求导，会使得噪声增加。所以三者相比之下，狭缝LSF是比较稳定的测量MTF方法，德国trioptics公司使用的就是狭缝LSF测量法。 </p>

<h2 id="mtf分析实例:754679329a565ba3bd547f760dbb0738">MTF分析实例</h2>

<p>因为暂时没有模拟LSF图像的方法，下面笔者以点光源模拟成像和模拟计算MTF的数据来说明如何分析MTF。</p>

<p>选用镜头规格 <sup>1</sup>&frasl;<sub>2</sub>.6” I.H=3.432mm, F2.2, FL=4.42mm。</p>

<p>镜头放置在测量设备上（假设点光源在物体侧），设备机构如果事先有校正，测试开始后，应该很容易找到中心视场成像如下，随后设备会测量中心点像的离焦MTF，过程模拟如下：</p>

<p>a) 搜索到中心视场的点像，记录下此时defocus位置P0</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/mtf_test_intro/defocus_p0.jpg" alt="Defocus P0" /></p>

<p>b) 从P0 - 0.03位置开始摄取点像并计算MTF，假定step为0.01，测到P0 + 0.03。
（如果焦深很大，则P0两侧的defocus测定范围也要相应的扩大）</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/mtf_test_intro/defocus_spots.png" alt="Defocus Spots" /></p>

<p>7个defocus位置的MTF近似绘制MTF through focus曲线，设备软件计算出peak位置，并移动物镜对焦在peak位置上</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/mtf_test_intro/mtf_thru_focus.png" alt="MTF Through Focus" /></p>

<p>如果测量的是mtf vs field(0~1.0field，step 0.1field)，那么接下来测量过程如下：</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/mtf_test_intro/mtf_vs_field.png" alt="MTF vs. field" /></p>

<p>因为是点光源，所以T.S两个方向一次全部计算出来了，各视场MTF连起来曲线如下:</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/mtf_test_intro/mtf_vs_focus_curve.png" alt="MTF vs. focus" /></p>

<p>这个过程同样可以输出mtf vs frequency如下：</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/mtf_test_intro/mtf_vs_freq.png" alt="MTF vs. freq" /></p>

<p>如果要测量周边视场的mtf through focus曲线，则每个视场都要重复步骤b。</p>

<p>1.0field T.S example</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/mtf_test_intro/defocus_spots2.png" alt="Defocus spots example" /></p>

<p>各defocus的MTF连起来即为下图。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/mtf_test_intro/defocus_mtf_curve.png" alt="Defocus MTF curves" /></p>

<p>以上即为MTF实际测量的过程，原理很简单。鉴于以上MTF及点像是镜头设计值输出，下面输出一组包含tolerance的数据。为简化输出，field设定+/- 0.8, +/-0.6, +/-0.4, center。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/mtf_test_intro/defocus_spots_sample.png" alt="Defocus Spots Sample" /></p>

<p>其MTF vs field数据如下，是颗很差的镜头，尤其是T方向差。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/mtf_test_intro/mtf_vs_field_sample.png" alt="MTF vs. filed smaple" /></p>

<p>各视场MTF through focus数据如下：</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/mtf_test_intro/mtf_through_focus_sample1.png" alt="MTF thru focus 1" /></p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/mtf_test_intro/mtf_through_focus_sample2.png" alt="MTF thru focus 2" /></p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/mtf_test_intro/mtf_through_focus_sample3.png" alt="MTF thru focus 3" /></p>

<p>从mtf through focus曲线看出，像面tilt是MTF NG的主要原因。以+/- 0.8F为例，我们看看through focus的点像如何。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/mtf_test_intro/mtf_thru_focus_spots_samples.png" alt="MTF thru focus spots" /></p>

<p>从上图很容易看出，Y轴方向代表T，-0.8field在defocus-0.01是Y方向光线汇聚最佳的，而+0.8field在defocus 0.02的时候Y方向最汇聚，但是X方向却发散，意味着S方向在defocus +0.01汇聚最佳。要矫正+/- 0.8field，可以选择倾斜sensor，使sensor image plane 正方向落在defocus 0.01mm位置，而负方向落在-0.01mm，此时系统拥有最佳的组合解析力。因此镜头的测试不仅可以鉴别镜头的质量也可以为成像系统的设计增加更多依据。</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://allincamera.github.io/post/cdaf/">
        反差式对焦评价函数的频率特性考虑
      </a>
    </h1>

    <span class="post-date">Thu, Apr 7, 2016</span>

    

<p>当前手机成像系统中普遍使用反差式对焦系统，也就是计算当前图像的锐度，依照图像锐度与镜头位置的关系，寻找最锐度最高的镜头位置作为合焦位置的方法。在搜索策略上各厂商基本相似，在锐度提取也就是清晰度的计算上，应该根据光学特性的不同做差异化设计。</p>

<p>基于芯片硬件设计的方便性考虑，大部分的锐度评价函数都设计为一个m x n算子，记作P，算子的填充决定其特性：一般是高通，或者是带通滤波器。如果图像记作G，那么图像的锐度S可以表示为：
S = P * G  P 对 G 做卷积。</p>

<h3 id="高通与带通滤波器的优缺点:21488b96ffcd9dfd757627af1ef7cae6">高通与带通滤波器的优缺点</h3>

<p>高通滤波器对高频成分很敏感，当成像系统离焦不远时，图像高频成分很容易被提取出来，随着镜头的移动，计算出锐度的差异很明显。但是当图像离焦很远时，比如对焦的物体消失，背景物体又在远处，高通滤波器对图像的响应就不明显，这样镜头移动计算出的锐度变化就不明显，造成搜索失败。</p>

<p>如下图</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/bandpass_filter.gif" alt="Band Pass Filter" /></p>

<p>当锐度值在曲线两侧，无论镜头如何移动，变化都非常不明显，这样搜索算法就很难工作，这个评价函数就是不合适的。为了解决这个问题，就需要在设计滤波器P的时候，让低频成分多通过一些。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/P1P2.gif" alt="Low Pass Filter" /></p>

<p>蓝色的曲线是滤波器P1的频率响应，绿色的曲线是滤波器P2的频率响应，相比可见，绿色曲线可以让更多低频成分通过。</p>

<p>选择一个滤波器后，需要根据实际图像进行计算仿真，画出这个滤波器对不同离焦程度图像卷积所产生的锐度值</p>

<pre><code class="language-matlab">	  kern = [-1 -2 -3 -5 -8;8 5 3 2 1];%c    
	%  kern = [-1  0  1; -2 0  2; -1  0  1];%r
	%     kern = [ -1 -1 -1;-1 0 1;1 1 1];%g
	%     kern = [ -1 -1 -1;-1 8 -1;-1 -1 -1];%b
	%    kern = [ 7 5 -1 0 -1 ;1 0 -5 1 -7]; %m
	     kern = [ 6 0 6 0 0 ;0 -12 0 0 0];%k
	%    kern = [-5 -4 0 4 5;-8 -10 0 10 8;10 -20 0 20 0;-8 -10 0 10 8;-5 -4 0 4 5]%y
</code></pre>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/filters_comparison.gif" alt="Filter Comparison" /></p>

<p>上图就是上面的各个滤波器的仿真结果，横坐标是离焦程度，从0到25，离焦程度逐步变大。纵坐标是计算出的锐度值，1表示最大。通过这个曲线，可以看出对不同离焦程度图像滤波器的响应，依据响应曲线的特点进行选择。</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://allincamera.github.io/post/pdaf/">
        片上相差式自动对焦原理
      </a>
    </h1>

    <span class="post-date">Thu, Apr 7, 2016</span>

    <p>相差式自动对焦与反差式自动对焦是现在手机成像系统中两大主要自动对焦方式。相比反差式自动对焦，相差式自动对焦只需要一次计算，就可以完成对焦。</p>

<p>当前比较流行的是片上相差自动对焦（on chip phase detection autofocus）, 在生产sensor的时候，把某些用于相位检测像素遮住左边一半或者右边一半，如下图</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/focus_pixel_array.png" alt="Focus Pixels Array" /></p>

<p>上图只是示意图，各个厂商的半掩模的工艺各有不同，在对IR filter或者microlens的处理上也不相同，但是基本的原理都是让图像形成左右两幅类似人眼的不同光学通路的图像。</p>

<p>这样左右侧的相位检测像素就会产生这样的图像：</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/focus_pixel_sequence.png" alt="Phase diff in focus pixels" /></p>

<p>数字化以后就产生了两个序列。</p>

<p>图像聚焦时，两个序列做互相关运算产生的数值变小，图像离焦时，两个序列做互相关产生的数值变大。如果对相机模组进行校准&mdash;-针对一个固定图形的高频图像移动镜头，计算互相关运算产生的数值，记录下来成为基准表。在相机工作时，根据实时计算的互相关数值，通过查找基准表，就可以知道当前的离焦程度，从而找到移动方向和移动到什么位置。</p>

<p>数学推导简化起来就是如下公式：</p>

<p>左右两个图像产生的数列做互相关，得到一个对焦函数，可以把相差与镜头的偏移量变成一一对应关系。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/pdaf_func.png" alt="PDAF func" /></p>

<p>实际工程上计算得到的结果就如下边图中所示，5x5窗口，每个窗口里边的统计数据包括两个部分，高16位是相位差，低16位是置信度。在平坦区域，置信度低，在细节丰富的区域，置信度高（300）。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/cdaf_pdaf/pdaf_output.png" alt="PDAF output" /></p>

<p>通过固定图卡校准可以得到lens 偏移量和相差的对应数组：</p>

<blockquote>
<p>PDAF_Calibration[][2] = {{1,1},{2,3},{3,5},{4,7},{5,9},{6,10},{7,11},{8,12},{9,13},{10,14},{11,15},{12,16},{13,17},{14,18},{15,19},{16,20}, {20,30},{24,40},{28,47},{32,50},{40,70},{48,80},{56,96},{64,110},{80,138},{96,160},{112,180},{128,210}};</p>
</blockquote>

<p>所以当AF开始工作时，通过实时计算得到相差值，eg： 210， 那么对应移动lens的距离，就是128,如果得到相差值是-210，就移动lens向反方向128个单位。</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://allincamera.github.io/post/resolution_2/">
        解析力 （2）空间采样 和 奈奎斯特
      </a>
    </h1>

    <span class="post-date">Wed, Apr 6, 2016</span>

    <p>上一篇也说到MTF曲线的时候横坐标是空间频率。一般使用黑白交替的线对来表示空间频率。而空间频率的单位一般是线对每毫米(lp/mm)，周期每毫米（cycles/mm）,周期每像素（cycles/pixel），线宽每图像高（LW/PH Line Widths per Picture Height），线对每图像高（lp/ph）。其中lp/mm是目前使用最多的单位。cycles/pixel是在数码相机中的成像系统的。数码相机下一个像素就是1 cycles/pixel，两个像素就是0.5 cycles/pixel，4个像素是0.25 cycles/pixel.其它单位的计算如下，纵向是已知横向是未知。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/resolution_2/freq_units.jpg" alt="Freq Units" /></p>

<p>当知道了空间频率的单位之后又有了一个问题。到底用什么样的空间频率去评价MTF合适呢？这个时候经常能看到一个名词奈奎斯特（Nyquist）频率，这是来自采样定律。奈奎斯特和成像有什么关系呢？
数字相机的Sensor在成像过程中就相当与对镜头成的模拟像进行空间数字采样。
奈奎斯特采样定理是指在进行模拟与数字信号的转换过程中，当采样频率大于信号中最高频率的2倍时，采样之后的数字信号完整地保留了原始信号中的信息，但是一般实际应用中保证采样频率为信号最高频率的5～10倍。数码相机的Nyquist取决于pixel 的大小.根据前面给出的空间频率单位转换公式。</p>

<p>对于给定的Pixel size的sensor</p>

<blockquote>
<p>Nyquist 频率= 1000[µm]/（pixel_pitch [µm]X2）</p>
</blockquote>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/resolution_2/nyquist_sample.jpg" alt="Nyquist Sample" /></p>

<p>评价MTF使用的奈奎斯特频率（Nyquist频率）是离散信号系统采样频率的一半，也就是一个方向上的像素数一半。采样定理指出，只要离散系统的奈奎斯特频率高于被采样信号的最高频率或带宽，就可以避免混叠现象。但这只是理论上。我们先看下一个接近奈奎斯特频率的频率的采样过程。当然下面的成像过程都是基于一个理想的没有MTF衰减的镜头的情况下。我们实际的像素对正弦图卡的采样过程可以模拟为</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/resolution_2/wave_pixel_image.png" alt="Wave-Pixel-Image" /></p>

<p>而在采样理论中的采样过程中类似下图。两个采样点之间的周器和数字成像系统的像素一样。但是希望采样点能够尽量小。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/resolution_2/wave_idea_sample_image.png" alt="Wave-Idea-Samples-Image" /></p>

<p>采样定理中的只要小于奈奎斯特频率频率都可以被采样还原是有两个条件的。</p>

<p>1采样点的尽量小，而我们的像素大小实际上接近于</p>

<p>2重建信号的过程需要以一个低通滤波器或者带通滤波器将在奈奎斯特频率之上的高频分量全部滤除，同时还要保证原信号中频率在奈奎斯特频率以下的分量不发生畸变。</p>

<p>这两者在图像系统中都很难满足。因此很多时候即使采样过程中信号的最大频率小于奈奎斯特频率频率依然无法很好的得到采样还原。</p>

<p>即使选取一个略小于奈奎斯特频率频率的正弦图卡。当发生相差的时候像素就很有可能无法正确采样出来实际图卡的原来的形状。但是多数情况下使用理想的采样模型是可以将原有信号正确的采样出来的，下面两张图是相差的影响对实际像素的采样影响。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/resolution_2/wave_pixel_image2.png" alt="Wave-Pixel-Image-Phase-Diff" /></p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/resolution_2/wave_idea_sample_image2.png" alt="Wave-Idea-Samples-Image-Phase-Diff" /></p>

<p>前面提到当频率高于奈奎斯特频率的时候会产生混叠现象。混叠(aliasing) 在信号也称为叠频；在成像上称为叠影，叠影会产生伪纹也就是平常说的摩尔纹。下面的就是一个高于Nyquist频率的频率产生伪纹的采样情况。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/resolution_2/wave_pixel_sample3.png" alt="Wave-Pixel-High-Freq" /></p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/resolution_2/overlap.jpg" alt="Overlap" /></p>

<p>从采样的一般规律来说如果要消除上面这些相差和混叠的影响就需要提高采样的频率。对于常用的采样频率我们可以参考示波器。示波器是一个常用的采样系统，一般示波器的采样频率在被采样频率的5到8倍。但是数字成像系统不同于示波器的是采样的频率在感应器被确定了之后就已经定了。更多的我们是希望知道原有图像在什么频率下分析成像系统的品质更合适。图像采样的过程中肯定是频率越低的图像越清晰。但是一般什么频率的图像是采样的清晰度是应该可以很好的分辨呢。根据如下图一样的仿真和经验，一般能很好分辨的频率在每个线对4个像素左右，也就是1/2奈奎斯特频率.。下面两张图是在1/2 Nyquist频率采样的情况</p>

<p>没有相差的1/2 Nyquist频率采样的情况</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/resolution_2/nyquist_sample_nodiff.png" alt="No diff" /></p>

<p>有相差的1/2 Nyquist频率采样的情况</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/resolution_2/nyquist_sample_1by2.png" alt="Diff 1/2" /></p>

<p>我们可以看到在1/2 Nyquist下即使有相差，也可以基本上还原原有图像的形状了。但是这只是仿真像素的采样过程中，在实际测试过程中由于镜头，噪声和测试环境影响在1/2奈奎斯特频率附近测试MTF值很不稳定。因此也经常选取更低1/4奈奎斯特频率作为MTF测试值所选取的频率。不过MTF的评价并不是简单的看一个频率就可以评价一个成像系统的效果，MTF曲线是一个整体。下一篇中将介绍怎么来通过SFR算法得到MTF曲线评价一个手机模组。</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://allincamera.github.io/post/resolution_1/">
        解析力 （1 ）MTF SFR
      </a>
    </h1>

    <span class="post-date">Wed, Apr 6, 2016</span>

    

<h2 id="基本概念:a964ef45263c6c27f345fdb68fdb6cb3">基本概念</h2>

<p>成像系统的解析力一直是摄像头最关键的指标之一。所有用户拿到一张照片的时候首先看到的是照片清楚不清楚，这里的清楚说得就是解析力。但是如何评价一个成像系统的解析力也是大家一直在探讨的问题。目前主流的办法主要有三种TV line检测，MTF检测，和SFR 检测。</p>

<h3 id="tv-line:a964ef45263c6c27f345fdb68fdb6cb3">TV line</h3>

<p>TV line主要用于主观测试，也有一些读取TV line的软件如HYRes。但是总体来说没有一个具体的标准。大多数公司是以人的读取为标准。不同人的读取，以及状态的不同都会导致读取值的不稳定。而且如ISO12233 chart 实际上我们读出的线对数只能代表读出位置的状况。尤其中心的TV line跨度很大，很难反映一个成像系统</p>

<h3 id="mtf:a964ef45263c6c27f345fdb68fdb6cb3">MTF</h3>

<p>MTF是Modulation Transfer Function的英文简称，中文为调制传递函数。是指调制度随空间频率变化的函数称为调制度传递函数。个传递函数最开始是为了说明镜头的能力。在各个摄像头镜头中经常采用MTF描述镜头的MTF曲线，表明镜头的能力。这些曲线是通过理想的测试环境下尽量减少其它系统对镜头的解析力的衰减的情况下测试得出的。但是其实MTF也可以涵盖对整个成像系统的解析力评价。在这里咱们就不多讨论这个问题了，如果有兴趣可以开另外一篇文章讨论。</p>

<h3 id="sfr:a964ef45263c6c27f345fdb68fdb6cb3">SFR</h3>

<p>SFR是 spatial frequency response (SFR) 主要是用于测量随着空间频率的线条增加对单一影像的所造成影响。简言之SFR就是MTF的另外一种测试方法。这种测试方法在很大程度上精简了测试流程。SFR的最终计算是希望得到MTF曲线。SFR的计算方法和MTF虽然不同但是在结果上是基本一致的</p>

<h2 id="测量方法:a964ef45263c6c27f345fdb68fdb6cb3">测量方法</h2>

<p>现在我们来看一下传统的MTF是怎么测量出来的，后面我们再针对SFR的原理和MTF的关系进行一些介绍。在以后的文章中我们在介绍一些MTF和SFR测试需要注意的问题。
从上面我们知道MTF是描述不同空间频率下的调制函数。那么什么是空间频率呢？通常，描述频率的单位是赫兹(Hz)，比如50Hz、100MHz之类的。但空间频率的表述习惯用“每毫米线对”。（LP/mm），就是每毫米的宽度内有多少线对。每两条线条之间的距离，以及线条本身的宽度之比是个定值，目前我国分辨率的标板规定，这个定为公因子是20√10≈1.122等比级数。一般MTF的计算离不开线对。下面这个图就是一张不同频率的线对测试图 ，可以看到图卡本身黑色和白色的对比是很清楚的。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/resolution_1/lp_demo1.gif" alt="LP Demo1" /></p>

<p>实际拍摄的时候，就像上图一样频率越高（越细）的线对就越模糊。这就是我们实际拍摄场景中到一定小的纹理的就拍摄不清楚的原因。而MTF的计算就是计算线对间最亮和最暗线对的对比度。计算公式为</p>

<blockquote>
<p>MTF = (最大亮度 - 最小亮度) / (最大亮度 + 最小亮度)</p>
</blockquote>

<p>所以MTF的计算不会出现大于1的情况。像下面的图表示的这样，当我们测试了很多不同频率下的MTF值。通过将这些值和空间频率进行一一的对照。通过这条曲线我们就能知道现在的成像系统在什么样的空间频率下的对比度如何。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/resolution_1/lp_demo2.jpg" alt="LP Demo2" /></p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/resolution_1/lp_demo3.jpg" alt="LP Demo3" /></p>

<p>SFR是怎么测试和计算的呢。首先SFR不需要拍摄不同的空间频率下的线对。它只需要一个黑白的斜边（刀口）即可换算出约略相等于所有空间频率 下的MTF。如何通过一个斜边计算出大家可以去看下ISO12233-2000那篇文档，里面说的很详细。具体的流程如下图。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/resolution_1/sfr_calc.jpg" alt="Calculate SFR" /></p>

<p>其实简单得来说呢，SFR是通过这条斜边的图进行超采样的到一条更加细腻的黑白变换的直线（ESF）。然后通过这条直线求导得到直线的变化率（LSF）。然后对将这个变化率进行FFT变换就能得到各个频率下的MTF的值。这里面的ESF，LSF，都是什么呢?</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/resolution_1/esf_lsf_mtf.jpg" alt="Calculate SFR" /></p>

<p>点扩展函数PSF(Point Spread Function)、线扩展函数LSF(Line Spread Function)和边缘扩展函数ESF(Edge Spread Function)是SFR 计算中的几个重要的概念。点扩展函数PSF是点光源成像后的亮度分布函数，如下图所示，用PSF(X,Y)表示。点扩展函数是中心圆对称的，通常以沿x轴的亮度分布PSF(X,Y)作为成像系统的点扩展函数。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/resolution_1/psf.jpg" alt="Calculate SFR" /></p>

<p>当获取点光源像的亮度分布函数PSF(X,Y)后，对其进行二维傅里叶变换即可得MTF (u，v)。因此，从理论上讲，从PSF也是获取MTF的一个方法。但是，在实际的应用中，由于地面点光源强度很弱，此方法一般较少采用。相对于PSF来说，LSF的能量得到了一定程度的加强。SFR计​算MTF就通过ESF来得到LSF然后进行FFT得到MTF各个频率的值的。这几者之间的关系如下图。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/resolution_1/psf_esf_lsf_mtf.jpg" alt="Calculate SFR" /></p>

<p>说实话光从这几个数学公式还是不好理解为什么ESF可以求出MTF。换一种角度理解LSF就是一条线上（ESF） 的变化的过称。对于任意一条线由黑变白的过程是由不同频率的黑白线对组成。因此可以反过来通过分析一条线得到这些频率下的　（FFT）。当然这只是一种朴素的理解。后面的文章中会有实际使用的MTF和SFR图卡和测试环境和问题进行进一步讨论</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/resolution_1/bottom.jpg" alt="Calculate SFR" /></p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://allincamera.github.io/post/hdr_sensor_intro/">
        HDR Sensor 原理介绍
      </a>
    </h1>

    <span class="post-date">Tue, Apr 5, 2016</span>

    

<p>在介绍HDR Sensor原理之前首先讨论为什么需要HDR Sensor.</p>

<h2 id="什么是sensor的动态范围-dynamic-range:8adb1af61272cd03fe2458d7ffdec523">什么是sensor的动态范围（dynamic range）？</h2>

<p>sensor的动态范围就是sensor在一幅图像里能够同时体现高光和阴影部分内容的能力。
用公式表达这种能力就是：</p>

<blockquote>
<p>DR = 20log10（i_max / i_min); //dB</p>
</blockquote>

<p>i_max 是sensor的最大不饱和电流&mdash;-也可以说是sensor刚刚饱和时候的电流
i_min是sensor的底电流（blacklevel） ；</p>

<h2 id="为什么hdr在成像领域是个大问题:8adb1af61272cd03fe2458d7ffdec523">为什么HDR在成像领域是个大问题？</h2>

<p>在自然界的真实情况，有些场景的动态范围要大于100dB。</p>

<p><strong>人眼的动态范围可以达到100dB。</strong></p>

<p>Sensor 的动态范围： <strong>高端的 &gt;78 dB; 消费级的 60 dB 上下；</strong></p>

<p>所以当sensor的动态范围小于图像场景动态范围的时候就会出现HDR问题&mdash;-不是暗处看不清，就是亮处看不清，有的甚至两头都看不清。
<img src="https://raw.githubusercontent.com/allincamera/imgur/master/hdr_sensor_intro/dark_blur.png" alt="Dark Blur Photo" />
暗处看不清&ndash;前景处的广告牌和树影太暗看不清。
<img src="https://raw.githubusercontent.com/allincamera/imgur/master/hdr_sensor_intro/bright_blur.png" alt="Bright Blur Photo" />
亮处看不清&ndash;远处背景的白云变成了一团白色，完全看不清细节。</p>

<h2 id="解决hdr问题的数学分析:8adb1af61272cd03fe2458d7ffdec523">解决HDR问题的数学分析</h2>

<p>根据前边动态范围公式</p>

<blockquote>
<p>DR = 20log10（i_max / i_min); //dB</p>
</blockquote>

<p>从数学本质上说要提高DR，就是提高i_max，减小 i_min；</p>

<ul>
<li><p>对于10bit输出的sensor, i_max =1023，i_min =1, 动态范围DR = 60；</p></li>

<li><p>对于12bit输出的sensor， DR = 72；</p></li>
</ul>

<p>所以从数学上来看，提高sensor 输出的bit width就可以提高动态范围，从而解决HDR问题。可是现实上却没有这么简单。提高sensor的bit width导致不仅sensor的成本提高，整个图像处理器的带宽都得相应提高，消耗的内存也都相应提高，这样导致整个系统的成本会大幅提高。所以大家想出许多办法，既能解决HDR问题，又可以不增加太多成本。</p>

<h2 id="解决hdr问题的5种方法:8adb1af61272cd03fe2458d7ffdec523">解决HDR问题的5种方法</h2>

<p>从sensor的角度完整的DR 公式：</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/hdr_sensor_intro/dr_formula.png" alt="DR Formula" /></p>

<ul>
<li>Qsat ：Well Capacity   idc:  底电流，tint：曝光时间，σ:噪声。</li>
</ul>

<h3 id="方法1-提高qsat-well-capacity:8adb1af61272cd03fe2458d7ffdec523">方法1：提高Qsat  &ndash;Well capacity 。</h3>

<p>就是提高感光井的能力，这就涉及到sensor的构造，简单说，sensor的每个像素就像一口井，光子射到井里产生光电转换效应，井的容量如果比较大，容纳的电荷就比较多，这样i_max的值就更大。普通的sensor well只reset一次，但是为了提高动态范围，就产生了多次reset的方法。
通过多次reset，imax增加到i‘max，上图就是current to charge的转换曲线。
但这种方法的缺点是增加FPN，而且sensor的响应变成非线性，后边的处理会增加难度。</p>

<h3 id="方法2-多曝光合成:8adb1af61272cd03fe2458d7ffdec523">方法2：多曝光合成</h3>

<p>本质上这种方法就是用短曝光获取高光处的图像，用长曝光获取阴暗处的图像。有的厂家用前后两帧长短曝光图像，或者前后三针长、中、短曝光图像进行融合</p>

<p>&rdquo;&rsquo;
    If (Intensity &gt; a) intensity = short_exposure_frame;
    If (Intensity &lt; b) intensity = long_exposure_frame;
    If (b&lt;Intensity &lt;a) intensity = long_exposure_frame x p + short_exposure_frame x q;
&ldquo;&rsquo;</p>

<p>当该像素值大于一个门限时，这个像素的数值就是来自于短曝光，小于一个数值，该像素值就来自于长曝光，在中间的话，就用长短曝光融合。这是个比较简化的方法，实际上还要考虑噪声等的影响。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/hdr_sensor_intro/curve_multi_frame_current.png" alt="Curve Multi Frame Current" /></p>

<p>Current to charge曲线显示：imax增加a倍。
这种多帧融合的方法需要非常快的readout time，而且即使readout时间再快，多帧图像也会有时间差，所以很难避免在图像融合时产生的鬼影问题。尤其在video HDR的时候，由于运算时间有限，无法进行复杂的去鬼影的运算，会有比较明显的问题。于是就出现了单帧的多曝光技术。</p>

<h3 id="方法3-单帧空间域多曝光:8adb1af61272cd03fe2458d7ffdec523">方法3：单帧空间域多曝光。</h3>

<p>最开始的方法是在sensor的一些像素上加ND filter，让这些像素获得的光强度变弱，所以当其他正常像素饱和的时候，这些像素仍然没有饱和，不过这样做生产成本比较高，同时给后边的处理增加很多麻烦。所以下面的这种隔行多曝光方法更好些。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/hdr_sensor_intro/single_frame_multi_exp.png" alt="Single Frame Multi Exposure" /></p>

<p>如上图所示,两行短曝光，再两行长曝光,然后做图像融合，这样可以较好的避免多帧融合的问题，从而有效的在video中实现HDR。同时由于video的分辨率比still要低很多，所以这个方法所产生的分辨率降低也不是问题。这个方法是现在video hdr sensor的主流技术。</p>

<h3 id="方法4-logarithmic-sensor:8adb1af61272cd03fe2458d7ffdec523">方法4：logarithmic sensor</h3>

<p>实际是一种数学方法，把图像从线性域压缩到log域，从而压缩了动态范围，在数字通信里也用类似的技术使用不同的函数进行压缩，在isp端用反函数再恢复到线性，再做信号处理。</p>

<p>缺点一方面是信号不是线性的，另一方面会增加FPN，同时由于压缩精度要求对硬件设计要求高。</p>

<h3 id="方法5-局部适应-local-adaption:8adb1af61272cd03fe2458d7ffdec523">方法5：局部适应 local adaption</h3>

<p>这是种仿人眼的设计，人眼会针对局部的图像特点进行自适应，既能够增加局部的对比度，同时保留大动态范围。这种算法比较复杂，有很多论文单独讨论。目前在sensor 端还没有使用这种技术，在ISP和后处理这种方法已经得到了非常好的应用。</p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/hdr_sensor_intro/2_5_hdr.png" alt="After HDR" /></p>

<p>上图就是用方法2 + 方法5处理后的HDR图像。亮处与暗处的细节都得到了很好的展现。</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://allincamera.github.io/about/">
        关于本站
      </a>
    </h1>

    <span class="post-date">Tue, Apr 5, 2016</span>

    <p>分享有关成像的软硬件及算法知识，交流相关开发经验，欢迎投稿</p>

<p>Powered by <a href="https://gohugo.io/">https://gohugo.io/</a> &amp; <a href="https://disqus.com">https://disqus.com</a></p>

<p>Here we discuss camera, welcome to add comments in page or send post <a href="https://github.com/allincamera/allincamera.github.io.git">https://github.com/allincamera/allincamera.github.io.git</a></p>

<p><img src="https://raw.githubusercontent.com/allincamera/imgur/master/about.png" alt="About This Site" /></p>

  </div>
  
</div>
</div>

  </body>
</html>
